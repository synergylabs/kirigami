{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/haozhezh/Documents/Research/CMU/AudioFeaturizationAttack/Kirigami_Publish/Kirigami-private-audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haozhezh/Kirigami-private-audio/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'/Users/haozhezh/Documents/Research/CMU/AudioFeaturizationAttack/Kirigami_Publish/Kirigami-private-audio'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure in the project's roots' directory\n",
    "%cd ../\n",
    "%pwd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import soundfile\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Speech Detector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def fft_features_with_tags(wav_form, speech_start, speech_end, speech_start2, window_size=256, non_overlap=128):\n",
    "  _, _, stft = sp.signal.stft(x=wav_form, fs=16000, nperseg=window_size, noverlap=non_overlap)\n",
    "  speech_start_window = (speech_start - window_size + non_overlap) // non_overlap\n",
    "  speech_end_window = (speech_end-window_size+non_overlap) // non_overlap\n",
    "  speed_start2_window = (speech_start2 - window_size + non_overlap) // non_overlap\n",
    "  features = []\n",
    "  stft = stft.transpose((1, 0))\n",
    "  tags = np.zeros(len(stft))\n",
    "  tags[:] = 0\n",
    "  tags[speech_start_window:speech_end_window] = 1\n",
    "  tags[speed_start2_window:] = 1\n",
    "  stft = np.abs(stft)\n",
    "  for fft in stft:\n",
    "    features.append(fft)\n",
    "  return features, tags\n",
    "\n",
    "def data_prep_io(interleave_wavs_path, interleave_csv_path):\n",
    "    train_folds = [1, 2, 3]\n",
    "    valid_folds = [4]\n",
    "    test_folds = [5]\n",
    "\n",
    "    interleave_df = pd.read_csv(interleave_csv_path)\n",
    "    train_x = []\n",
    "    train_tags = []\n",
    "    valid_x = []\n",
    "    valid_tags = []\n",
    "    test_x = []\n",
    "    test_tags = []\n",
    "\n",
    "    for idx, row in interleave_df.iterrows():\n",
    "        wav_form, sprate = soundfile.read(interleave_wavs_path + row['wav'])\n",
    "        fold_n = (idx % 5) + 1\n",
    "        features, tags = fft_features_with_tags(wav_form=wav_form, speech_start=row['wrd_start'], speech_end=row['wrd_end'], speech_start2=row['wrd_start2'])\n",
    "        if fold_n in train_folds:\n",
    "          train_x.extend(features)\n",
    "          train_tags.extend(tags)\n",
    "        if fold_n in valid_folds:\n",
    "          valid_x.extend(features)\n",
    "          valid_tags.extend(tags)\n",
    "        if fold_n in test_folds:\n",
    "          test_x.extend(features)\n",
    "          test_tags.extend(tags)\n",
    "    train_x, valid_x, test_x = np.asarray(train_x), np.asarray(valid_x), np.asarray(test_x)\n",
    "    train_tags, valid_tags, test_tags = np.asarray(train_tags), np.asarray(valid_tags), np.asarray(test_tags)\n",
    "    return train_x, train_tags, valid_x, valid_tags, test_x, test_tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "INTERLEAVE_WAV_PATH = './datasets/timit_interleave_on_esc50/'\n",
    "INTERLEAVE_CSV_PATH = './datasets/timit_interleave_on_esc50/timit_interleave_on_esc50_path.csv'\n",
    "\n",
    "batch_size = 128\n",
    "model_path = \"./results/filters/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "my_train_x, my_train_tags, my_valid_x, my_valid_tags, my_test_x, my_test_tags = \\\n",
    "data_prep_io(interleave_wavs_path=INTERLEAVE_WAV_PATH, interleave_csv_path=INTERLEAVE_CSV_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, feature_dim=129):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, xx):\n",
    "        xx = self.linear1(torch.nn.functional.normalize(xx, p=1.0, dim = 1))\n",
    "        return self.sigmoid(xx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the speech detector model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tensor_Xp_train = torch.stack([torch.Tensor(el) for el in my_train_x]).to(DEVICE)\n",
    "tensor_yp_train = torch.stack([torch.Tensor([el]) for el in my_train_tags]).to(DEVICE)\n",
    "dataset_p_train = TensorDataset(tensor_Xp_train, tensor_yp_train)\n",
    "loader_p_train = DataLoader(dataset_p_train, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "tensor_Xp_valid = torch.stack([torch.Tensor(el) for el in my_valid_x]).to(DEVICE)\n",
    "tensor_yp_valid = torch.stack([torch.Tensor([el]) for el in my_valid_tags]).to(DEVICE)\n",
    "dataset_p_valid = TensorDataset(tensor_Xp_valid, tensor_yp_valid)\n",
    "loader_p_valid = DataLoader(dataset_p_valid, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "tensor_Xp_test = torch.stack([torch.Tensor(el) for el in my_test_x]).to(DEVICE)\n",
    "tensor_yp_test = torch.stack([torch.Tensor([el]) for el in my_test_tags]).to(DEVICE)\n",
    "dataset_p_test = TensorDataset(tensor_Xp_test, tensor_yp_test)\n",
    "loader_p_test = DataLoader(dataset_p_test, shuffle=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "my_model = LogisticRegressionClassifier(feature_dim=129).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "bce = torch.nn.BCELoss()\n",
    "optim =  torch.optim.Adam(my_model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m validating_frequency \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m s:\n\u001B[0;32m---> 10\u001B[0m   \u001B[43mmy_model\u001B[49m\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     11\u001B[0m   total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     12\u001B[0m   total_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "max_epoch=30\n",
    "\n",
    "s = tqdm(range(0, int(max_epoch)),desc='Training Epochs')\n",
    "\n",
    "validating_frequency = 1\n",
    "\n",
    "for epoch in s:\n",
    "  my_model.train()\n",
    "  total_loss = 0\n",
    "  total_samples = 0\n",
    "  for tr_x, tr_y in loader_p_train:\n",
    "    y_pred = my_model(tr_x)\n",
    "    current_loss = bce(y_pred, tr_y)\n",
    "    current_loss.backward()\n",
    "    optim.step()\n",
    "    total_loss = total_loss + (current_loss.detach().cpu().numpy()) * tr_x.shape[0]\n",
    "    total_samples = total_samples + tr_x.shape[0]\n",
    "    batch_train_accuracy_privacy = (torch.sum((tr_y == 1) & (y_pred >= 0.5)) + torch.sum((tr_y  == 0) & (y_pred < 0.5)))/ (y_pred.shape[0])\n",
    "\n",
    "  if (epoch+1) % validating_frequency == 0:\n",
    "    my_model.eval()\n",
    "    total_y_pred = []\n",
    "    total_y_truth = []\n",
    "    for vl_x, vl_y in loader_p_valid:\n",
    "      y_pred = my_model(vl_x)\n",
    "      y_pred = (y_pred >= 0.5).long()\n",
    "      total_y_pred.extend(y_pred.cpu().numpy())\n",
    "      total_y_truth.extend(vl_y.cpu().numpy())\n",
    "\n",
    "    valid_accuracy = accuracy_score(total_y_truth, total_y_pred)\n",
    "    s.set_postfix(validation_accuracy = valid_accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Accuracy {valid_accuracy:.6f}\")\n",
    "    torch.save(my_model.state_dict(), f\"{model_path}/phoneme_filter_{epoch}.ckpt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_model.eval()\n",
    "total_y_pred = []\n",
    "total_y_truth = []\n",
    "for vl_x, vl_y in loader_p_test:\n",
    "  y_pred = my_model(vl_x)\n",
    "  y_pred = (y_pred >= 0.5).long()\n",
    "  total_y_pred.extend(y_pred.cpu().numpy())\n",
    "  total_y_truth.extend(vl_y.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(total_y_truth, total_y_pred)\n",
    "print(\"Test_Accuracy\", test_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
